---
title: "ANOVA"
subtitle: "![](logo.jpg){width=2in}"
author: "Dra. Stephanie Hereira Pacheco"
institute: "CICB, UATx"
date: 02-03-2024
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
nature:
  ratio: "16:9"
---


```{r, echo=FALSE}
htmltools::tagList(rmarkdown::html_dependency_jquery())
```


```{r, include=FALSE, warning=FALSE, eval=FALSE}
library(xaringanthemer)
style_mono_accent(
  base_color = "#5E2129",
  code_highlight_color = "#E3906F", 
  code_inline_color = "#0E2B54",
  text_font_size = "1.3rem",
  
)
```

```{r, xaringanExtra-clipboard, echo=FALSE}
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
  ),
  rmarkdown::html_dependency_font_awesome()
)

xaringanExtra::use_logo(
  image_url = "https://www.ciisder.mx/images/logos/logo_uatx_2019.png",
  position = xaringanExtra::css_position(top = "1em", right = "1em")
)

xaringanExtra::use_tile_view()

xaringanExtra::use_share_again()


```



```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
```

# Contenido

+ An√°lisis de varianza - ANOVA

+ Pruebas param√©tricas y supuestos estad√≠sticos

+ Supuestos del ANOVA

+ ANOVA de una v√≠a

    
---
## An√°lisis de varianza - ANOVA

<uw-blockquote> La t√©cnica de an√°lisis de varianza (**ANOVA**) tambi√©n conocida como an√°lisis factorial y desarrollada por Fisher en 1930

<uw-blockquote> Este an√°lisis constituye la herramienta b√°sica para el estudio del **efecto de una o m√°s variables independientes** (cada uno con dos o m√°s niveles) sobre la media de **una variable continua**. 

---
## An√°lisis de varianza - ANOVA

<uw-blockquote>El an√°lisis de varianza nos permite evaluar el efecto de $k$ variables independientes y su interacci√≥n en un experimento. 

<uw-blockquote> En el ANOVA las variables independientes se denominan **factores**.


---

## Pero antes de seguir....

--

- ¬øSabes qu√© es una variable dependiente e independiente?
--

- ¬øQu√© es un factor y cu√°les son sus niveles?
--

- ¬øQu√© es una variable continua y una variable discreta?
--

- ¬øQu√© es una interacci√≥n entre factores?

---
## An√°lisis de varianza - ANOVA

<uw-blockquote> Cuando existe una √∫nica variable independiente se denomina **Anova de un factor** (one way anova, en ingl√©s), cuando son dos **Anova de dos v√≠as** y cuando son m√°s de dos, se denomina como **Anova factorial**.

---
# Pruebas param√©tricas y supuestos estad√≠sticos

Antes de comenzar, todas las pruebas estad√≠sticas tienen supuestos...

--
- ¬øYa conocen alguna prueba estad√≠stica?
--


- ¬øQu√© supuestos tienen estas pruebas?

---
# Pruebas param√©tricas 

<uw-blockquote>  Se conoce como estad√≠stica param√©trica a aquella que se basa en el muestreo de una poblaci√≥n con una distribuci√≥n conocida (**normal**) y con  par√°metros fijos (**media poblacional, varianza o desviaci√≥n est√°ndar**). 

---
# Pruebas param√©tricas


|VENTAJAS    | DESVENTAJAS     | 
|-------------------|-------|
| - Sensibles a rasgos de los datos recolectados  | - M√°s complicadas de calcular | 
| - Estimaciones probabil√≠sticas m√°s exactas       |- Solo se pueden aplicar si se cumplen sus supuestos   | 
| - Tienen una mayor eficiencia y poder estad√≠stico      | - Los datos que se pueden observar son limitados| 

---
# Pruebas param√©tricas

|TIPO    |   PRUEBA     | 
|:-------------------:|:-------:|
| Comparaci√≥n de 2 grupos	  |   t de Student/Welch  | 
| Comparaci√≥n de >2 grupos       | **Anova**   | 
| Correlaci√≥n de dos variables     | Coeficiente de Pearson| 
| Variables cualitativas     | Prueba de Z| 

---
# Pruebas param√©tricas y supuestos estad√≠sticos

Los supuestos de las pruebas param√©tricas en general son:
--

- Distribuci√≥n conocida (**normal**): visual y pruebas num√©ricas.
--

- Homocedasticidad: visual y pruebas num√©ricas.
--

- Otros: tama√±o de la muestra, variables cuantitativas o continuas, outliers, aleatoriedad, independencia de las observaciones, linealidad. 

.

.center[*** Cada tipo de prueba param√©trica tiene sus propios supuestos***]
---


## Supuestos estad√≠sticos del ANOVA

Los supuestos del ANOVA son:

- **Distribuci√≥n normal de los residuales:** $\epsilon_{i,j} ‚àº N(0,\sigma^2)$. Es decir, los errores (residuos) deben distribuirse normalmente con media 0 y varianza constante.

- **Homocedasticidad**: Todos los grupos comparten la misma varianza poblacional $œÉ¬≤$.

- **Aleatoriedad e independencia**
 
- **Otros**: Mismo n√∫mero de observaciones por grupos, variable dependiente continua y variable independiente con tres o m√°s grupos o niveles. 

---

### Distribuci√≥n normal: m√©todos visuales

.pull-left[
```{r, echo=FALSE}
set.seed(123)

```

```{r, fig.align='center', fig.height=6}
data_normal<- rnorm(200)
hist(data_normal, col='steelblue', main='Normal')
```
]
.pull-right[
```{r, echo=FALSE}
set.seed(1254)
```

```{r, fig.align='center', fig.height=6}
data_no_normal<- rexp(100, rate=3)
hist(data_no_normal, col='red', main='No normal')
```
]
---
### Distribuci√≥n normal: m√©todos visuales

.pull-left[
```{r, echo=FALSE}
set.seed(123)

```

```{r, fig.align='center', fig.height=6}
plot(density(data_normal), main="Normal")
```
]
.pull-right[

```{r, fig.align='center', fig.height=6}
plot(density(data_no_normal), main="No Normal")
```
]

---
### Distribuci√≥n normal: m√©todos visuales

.pull-left[
```{r, echo=FALSE}
set.seed(123)

```

```{r, fig.align='center', fig.height=6}
qqnorm(data_normal)
qqline(data_normal)
```
]
.pull-right[

```{r, fig.align='center', fig.height=6}
qqnorm(data_no_normal)
qqline(data_no_normal)
```
]
---
### Distribuci√≥n normal: m√©todos num√©ricos

**Prueba de Shapiro‚ÄìWilk**

Hip√≥tesis

$ùêª_{0}$: Los datos provienen de una distribuci√≥n normal

$ùêª_{1}$: Los datos no provienen de una distribuci√≥n normal


$$W = \frac{\left(\sum_{i=1}^{n} a_i x_{(i)} \right)^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2}$$
D√≥nde:
$x_{(i)}$ = datos ordenados, $\bar{x}$ = media muestral, $a_{i}$ = constantes derivadas de la normal te√≥rica, $n$ = tama√±o de muestra
---
### Distribuci√≥n normal: m√©todos num√©ricos

**Prueba de Kolmogorov‚ÄìSmirnov**

Hip√≥tesis

$$ùêª_{0}: F(x) = F_{0}(x)$$ 

$$ùêª_{1}: F(x) \neq F_{0}(x)$$ 
$F(x)$ = distribuci√≥n acumulada de la muestra

$F_{0}(x)$ = distribuci√≥n te√≥rica

$$
D = \sup_{x} \left| F_n(x) - F_0(x) \right|
$$
D√≥nde: $F_n(x)$ es la funci√≥n emp√≠rica, $F_0(x)$ es la funci√≥n te√≥rica y supp la mayor diferencia te√≥rica. 



---
### Distribuci√≥n normal: m√©todos num√©ricos
.pull-left[
```{r, echo=FALSE}
set.seed(124)

```

```{r, fig.align='center', fig.height=6, message=FALSE, warning=FALSE}
shapiro.test(data_normal)
```


```{r, fig.align='center', fig.height=6, message=FALSE, warning=FALSE}
ks.test(data_normal, "pnorm")
```
]
.pull-right[

```{r, fig.align='center', fig.height=6}
shapiro.test(data_no_normal)

```

```{r, fig.align='center', fig.height=6, message=FALSE, warning=FALSE}
ks.test(data_no_normal, "pnorm")
```

]
---
### Probando heterocedasticidad
- M√©todos visuales = Pruebas estad√≠sticas de comparaci√≥n y modelos lineales

.pull-left[
```{r, echo=FALSE}
data("ToothGrowth")
data("iris")
```


```{r, fig.align='center', fig.height=5,fig.width=4, message=FALSE, warning=FALSE}
data("ToothGrowth")
boxplot(len ~ supp, data=ToothGrowth, col=c("red", "blue"), main="Dientes")
```

]
.pull-right[

```{r, fig.align='center', fig.height=5,fig.width=4, message=FALSE, warning=FALSE}
data("iris")
boxplot(Petal.Width ~ Species, data=iris, col=c("pink", "purple", "cyan"), main="Flores")
```

]
---
### Probando heterocedasticidad

.pull-left[
```{r, fig.align='center', fig.height=5,fig.width=4, message=FALSE, warning=FALSE}
aggregate(len ~ supp, data = ToothGrowth, var)
```
Ratio
```{r}
68.32 /  43.63
```

]
.pull-right[
```{r, fig.align='center', fig.height=5,fig.width=4, message=FALSE, warning=FALSE}
aggregate(Petal.Width ~ Species, data = iris, var)

```
Ratio
```{r}
r1<-0.03910612 / 0.01110612 #versicolor vs setosa
r2<-0.07543265 / 0.01110612 #virginca vs setosa
r3<-0.07543264 / 0.03910612 #virginica vs versicolor
cbind(r1,r2,r3)
```
]
---
### Probando heterocedasticidad
.pull-left[
```{r, fig.align='center', fig.height=5,fig.width=8, message=FALSE, warning=FALSE}
m1<-lm(len ~ supp, data=ToothGrowth)
par(mfrow = c(1, 2))
plot(m1, which=c(1,3))
```

]
.pull-right[

```{r, fig.align='center', fig.height=5,fig.width=8, message=FALSE, warning=FALSE}
m2<-lm(Petal.Width ~ Species, data=iris)
par(mfrow = c(1, 2))
plot(m2, which=c(1,3))
```

]
---
### Probando heterocedasticidad

**Prueba F para igualdad de varianzas (2 grupos)**

Hip√≥tesis

$$
H_0: \sigma_1^2 = \sigma_2^2 \qquad
H_1: \sigma_1^2 \neq \sigma_2^2
$$
$$
F = \frac{s_1^2}{s_2^2}
$$

D√≥nde:

${s_1^2}$ = varianza muestral del grupo 1

${s_2^2}$ = varianza muestral del grupo 2

---
### Probando heterocedasticidad

```{r, fig.align='center', fig.height=5,fig.width=8, message=FALSE, warning=FALSE}
var.test(len ~ supp, data = ToothGrowth) 
```

---
### Probando heterocedasticidad
Para m√°s de dos niveles

- **Prueba de Breusch‚ÄìPagan**: Eval√∫a si la varianza de los residuos depende de los predictores.

$$
H_0: \operatorname{Var}(\varepsilon_i) = \sigma^2
\qquad
$$

$$
BP = nR^2
$$

D√≥nde:

$n$ = tama√±o de la muestra

$R^2$ = coeficiente de determinaci√≥n de una regresi√≥n auxiliar
(residuos cuadrados contra los predictores)


---
### Probando heterocedasticidad


```{r, fig.align='center', fig.height=5,fig.width=8, message=FALSE, warning=FALSE}
lmtest::bptest(m2) #sobre un modelo
```


---
### Probando heterocedasticidad

- **Prueba de Levene**: Compara cu√°nto se dispersan los datos alrededor de su media  en cada grupo.

Si las dispersiones son similares ‚Üí homocedasticidad.

Hip√≥tesis: $H_{0}:\sigma^2 = \sigma^2_{1} = \sigma^2_{2} = ... = \sigma^2_{k}$

$$Z_{ij} = |Y_{ij} - \bar{Y}_{i\cdot}|$$

$Z_{ij}$ = distancia absoluta del dato respecto a la media de su grupo

$$F = \frac{MS_{\text{entre grupos}}}{MS_{\text{dentro de grupos}}}$$


---
### Probando heterocedasticidad
```{r, fig.align='center', fig.height=5,fig.width=8, message=FALSE, warning=FALSE}
library(car)
leveneTest(m2)
```

```{r, eval=FALSE}
?fligner.test
```


---
# An√°lisis de varianza de una v√≠a
- El **ANOVA de una v√≠a** es el tipo de an√°lisis que se emplea cuando los datos no est√°n pareados y se quiere estudiar si existen diferencias significativas entre las medias de una variable aleatoria continua en los diferentes niveles de otra variable cualitativa o factor.

+ Las hip√≥tesis contrastadas en un ANOVA de un factor son:
  - $H_{0}$ : No existen diferencias entre las medias de los grupos, es decir $\mu_{1}=\mu_{2}=....\mu_{k}$
  - $H_{1}$ : Al menos un par de medias es significativamente diferente una de la otra


---
# An√°lisis de varianza de una v√≠a
+ El ANOVA de una v√≠a, crea una comparaci√≥n entre la varianza en los datos que provienen de las diferencias entre grupos y la varianza en los datos que provienen de las diferencias dentro de los grupos.

 - $H_{0}$ : $\sigma^2_{entre} = 0$    $\rightarrow$ $\sigma^2_{entre}$ + $\sigma^2_{dentro}$ =  $\sigma^2_{dentro}$
 
 - $H_{1}$ : $\sigma^2_{entre} > 0$   $\rightarrow$ $\sigma^2_{entre}$ + $\sigma^2_{dentro}$ >  $\sigma^2_{dentro}$
 
 
 
---
# An√°lisis de varianza de una v√≠a
 
La forma en que el ANOVA hace esta comparaci√≥n es evaluando las razones de las varianzas entre grupos y las varianzas dentro de los grupos:

- a nivel de poblaci√≥n, si la raz√≥n es igual a 1, entonces las dos cosas son iguales

- si la raz√≥n es mayor que uno, entonces la varianza entre grupos es al menos un poco mayor que 0 y esto lo conocemos como el **el estad√≠stico F**.
---
# An√°lisis de varianza de una v√≠a

### El estad√≠stico F se calcula as√≠: 

$$
\begin{aligned}
F = \frac{\sigma^2_{entre}}{\sigma^2_{dentro}} = \frac{intervarianza}{intravarianza}= \frac{\frac{n\Sigma(\bar{x}_{k}-\bar{x})^2}{k-1}}{\frac{\Sigma(x_{k}-\bar{x_{k}})^2}{N-k}} = \frac{\frac{SCT}{k-1}}{\frac{SCE}{gl}}
\end{aligned}
$$


siendo $k$ los niveles, $N-k$ y $k - 1$ los grados de libertad y $SC$ la suma de cuadrados del error (intravarianza) y de los tratamientos (intervarianza).


---
# An√°lisis de varianza de una v√≠a : Tabla

|Fuente de variaci√≥n|Suma de cuadrados|Grados de libertad|Cuadros promedios|F|
|:--------:|:---------------:|:----------------:||:--------------:||:-:|
|Entre Grupos|SCT|k-1|CMT = SCT/k-1|F=CMT/CME|
|Dentro de los grupos o Error|SCE|N-k|CME = SCE/N-k|
|Total|STT|N-1|

---
# Ejemplo aplicado
```{r}
datos<- data.frame(Tratamiento1=c(-3.10,0.18,-0.72,0.09,-1.66),
                  Tratamiento2=c(7.28,3.06,4.74,5.29,7.88),
                  Tratamiento3=c(0.12,5.51,5.72,5.93,6.56),
                  Tratamiento4=c(8.18,9.05,11.21,7.31,8.83))
```
```{r, echo=FALSE, fig.align='center'}
knitr::kable(datos, align = "c")
```
---
## Probemos los supuestos
.pull-left[
### Normalidad

```{r, echo=TRUE, fig.align='center'}
ro1<- datos$Tratamiento1-mean(as.matrix(datos))
ro2<- datos$Tratamiento2-mean(as.matrix(datos))
ro3<- datos$Tratamiento3-mean(as.matrix(datos))
ro4<- datos$Tratamiento4-mean(as.matrix(datos))


shapiro.test(c(ro1,ro2, ro3, ro4))
```
]
.pull-right[
### Homocedasticidad
```{r}
library(tidyverse)
datost<- datos %>% pivot_longer(cols = everything(), names_to = "Tratamiento", values_to = "Valor")

lmtest::bptest(lm(Valor~Tratamiento, data = datost))


```
]


---
### Calculemos los grados de libertad

- Hay cuatro niveles entonces $k-1$ = 3
- Hay cinco observaciones en cada grupo entonces $N - k$ = (4*5)-3 = 16
- Los grados de libertad totales son $N - 1$ = 20-1 = 19

### Calculemos SCT
SCT = $n\Sigma(\bar{x}_{k}-\bar{x})^2$
```{r}
SCT1= 5*(mean(datos$Tratamiento1)-mean(as.matrix(datos)))^2
SCT2= 5*(mean(datos$Tratamiento2)-mean(as.matrix(datos)))^2
SCT3= 5*(mean(datos$Tratamiento3)-mean(as.matrix(datos)))^2
SCT4= 5*(mean(datos$Tratamiento4)-mean(as.matrix(datos)))^2

SCT= (SCT1+SCT2+SCT3+SCT4); SCT
```
 
---
### Calculemos SCE
.pull-left[SCE = $\Sigma(x_{k}-\bar{x_{k}})^2$
```{r}
SCE1= sum((datos$Tratamiento1-mean(datos$Tratamiento1))^2)
SCE2= sum((datos$Tratamiento2-mean(datos$Tratamiento2))^2)
SCE3= sum((datos$Tratamiento3-mean(datos$Tratamiento3))^2)
SCE4= sum((datos$Tratamiento4-mean(datos$Tratamiento4))^2)

SCE = (SCE1+SCE2+SCE3+SCE4); SCE
```
]

### Calculemos STT 

.pull-right[
STT = $\Sigma(x_{k}-\bar{x})^2$
```{r}
STT1= sum((datos$Tratamiento1-mean(as.matrix(datos)))^2)
STT2= sum((datos$Tratamiento2-mean(as.matrix(datos)))^2)
STT3= sum((datos$Tratamiento3-mean(as.matrix(datos)))^2)
STT4= sum((datos$Tratamiento4-mean(as.matrix(datos)))^2)

STT = (STT1+STT2+STT3+STT4); STT
```
]
---

.pull-left[
### Calculemos CMT
```{r}
CMT=SCT/(4-1);CMT
```
### Calculemos CME
```{r}
CME=SCE/(20-4);CME
```
]
.pull-right[
### Calculemos F

```{r}
valor_F= CMT/CME; valor_F
```

### Calculemos el valor p
```{r}
valor_p<-pf(valor_F, df1=4-1, df2=20-4, lower.tail=FALSE); valor_p

```
]
---
# Hag√°moslo en R
```{r}
anova_R<- aov(Valor~Tratamiento, data = datost)
summary(anova_R)    

```
#### Comparando...
```{r}
valor_F;valor_p
```
---

## Referencias y material suplementario

- [Advanced Statistics I 2021 Edition](https://bookdown.org/danbarch/psy_207_advanced_stats_I/differences-between-two-things.html#sign-binomial-test)

- [Pruebas param√©tricas y no param√©tricas](https://enviromigration.files.wordpress.com/2016/04/pruebas-paramc3a9tricas-y-no-parametricas.pdf)

- [Estad√≠stica param√©trica y no param√©trica](https://rstudio-pubs-static.s3.amazonaws.com/724751_c45a17f9e45f464c93e94f3fb0c6d340.html#16)

- [Pr√°cticos de bioestad√≠stica 2](https://derek-corcoran-barrios.github.io/AyduantiaStats/_book/t-student.html)
